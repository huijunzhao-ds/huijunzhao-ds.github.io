---
layout: post
title:  "Notes on CS224N 1: Introduction to word vectors"
---
Recently, I joined a 16-day NLP learning session held by an open-source organization [Datawhale](https://datawhale.club/). In the session, people group together to learn the awesome course [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html) by Stanford. I will keep updating my progress in the upcoming posts on this page, and my solutions to the assignments on my GitHub [repo](https://github.com/huijunzhao-ds/cs224).

The first lecture covers why we choose word vectors as a natural language representation, the loss function (likelyhood of occurrence of context words within a fixed-size window given the center word) and its optimization, and finally model variants including Skip-grams (SG), Continuous Bag of Words (CBOW), and model training technique, Negative Sampling. 

As a data scientist working on NLP in my daily life, I used to take vector representations of everything for granted, because vectors seem to be the most *natural* inputs to neural networks and for matrix computation. I never thought the other way around, like explained in the lecture.

So far, I have completed the first assignment, which is a jupyter notebook asking students to fill in some functions to implement the BOW algorithm, and to run the GenSim word2vec model pretrained using google-news-300 to explore the performance and some commonly-used functions in GenSim. As the kickoff assignment, this is supposed to be an easy one (and it is!), but meanwhile it reveals some deep thoughts on the word2vect algorithm via exploring the results. For example, the bias (gender, race, sexual orientation etc) implicit to word2vec model can be unobvious for a self-taught NLP developer like myself. Thanks to the course, now I have a much deeper understanding of the model.