---
layout: post
title:  "Notes on CS224N 2: Word Vectors"
---
This post is the second one in the sequence of Notes on the course [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html). In this post we focus on the word2vec model and related topics.

An intuitive word representation in NLP is the one-hot encoding vectors. The main issue here is the sparsity, namely we have no natural notion of similarity for two words because any two different vectors are orthogonal. This inspires the underlying idea of word2vec: __A word's meaning is given by the words that frequently appear close-by__.