---
layout: post
title:  "Notes on CS224N: Word2Vec and Named Entity Recognition"
---
This post is among the sequence of Notes on the course [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html). In this post we focus on the word2vec model and related topics.
## Word2Vec Algorithm 
An intuitive word representation in NLP is the one-hot encoding vectors. The main issue here is the sparsity, namely we have no natural notion of similarity for two words because any two different vectors are orthogonal. This inspires the underlying idea of word2vec: __A word's meaning is given by the words that frequently appear close-by__. As the name indicates, every word in the vocabulary (generated by a fixed corpus of text) is represented by a vector, so that the cosine similarity of the vectors for word $c$ (center word) and $o$ (outside word) is the probability of $o$ given $c$ (or vice versa). The vectors, or parameters in the word2vec model, are trained using Maximum Likelihood Estimation (MLE). The derived loss function (average negative log likelihood) for us to minimize during training is 

$$ J(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq 0} \log P(w_{t+j}|w_t;\theta) $$ 

where $m$ is the window size, a hyper-parameter of our model, indicating the window of outside words around a center word. Note that for each word $w$ in the vocabulary, we have two vectors $u_w$, $v_w$ for $w$ being a center word and an outside word respectively. Then we use the softmax as the predictive function

$$ P(o|c) = \frac{\exp (u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)} $$

The above predictive function is called __Skip-grams (SG)__ in some context, emphasizing that it predicts the outside words given the center word. On the other hand, __Continuous Bag of Words (CBOW)__ predicts the center word given (a bag of) outside words, where we switch the "o" and "c" in the formula above. 

## Optimization Techniques
Considering training efficiency, we apply techniques such as Stochastic Gradient Descent and Negative Sampling.


## Named Entity Recognition (NER)