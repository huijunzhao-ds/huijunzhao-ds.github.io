---
layout: post
title:  "Notes on CS224N: Word2Vec and Named Entity Recognition"
---
This post is among the sequence of Notes on the course [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html). In this post we focus on the __word2vec__ model and one of its many applications -- __Named Entity Recognition (NER)__.

## Word2Vec Algorithm 
An intuitive word representation in NLP is the one-hot encoding vectors. The main issue here is the sparsity, namely we have no natural notion of similarity for two words because any two different vectors are orthogonal. This inspires the underlying idea of word2vec: *A word's meaning is given by the words that frequently appear close-by*. As the name indicates, every word in the vocabulary (generated by a fixed corpus of text) is represented by a vector, so that the cosine similarity of the vectors for word $c$ (center word) and $o$ (outside word) is the probability of $o$ given $c$ (or vice versa). The vectors, or parameters in the word2vec model, are trained using Maximum Likelihood Estimation (MLE). The derived loss function (average negative log likelihood) for us to minimize during training is 

$$ J(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq 0} \log P(w_{t+j}|w_t;\theta) $$ 

where $m$ is the window size, a hyper-parameter of our model, indicating the window of outside words around a center word. Note that for each word $w$ in the vocabulary, we have two vectors $u_w$, $v_w$ for $w$ being a center word and an outside word respectively. Then we use the softmax as the predictive function

$$ P(o|c) = \frac{\exp (u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)} $$

## More Details

### SG and CBOW
The predictive function in the above section is called __Skip-grams (SG)__ , emphasizing that it predicts the outside words given the center word. On the other hand, __Continuous Bag of Words (CBOW)__ predicts the center word given (a bag of) outside words. To implement that, we can switch the "o" and "c" in the formula of $p(o|c)$ to compute $p(c|o)$ and replace the $P(w_{t+j}|w_t;\theta)$ with $P(w_t|w_{t+j};\theta)$ in the formula of $J(\theta)$.

### Co-occurrence matrix
One can capture the co-occurrence count matrix in two ways -- that within a window surrounding the center word, or that throughout each full document in the corpus.

The drawbacks of window-based co-occurrence mainly include the 3 followings.
- Co-occurrence is symmetric, meaning that it does not distinguish the left or right context;
- The storage size and dimensionality increase along with vocabulary size;
- High dimensionality also results in sparsity issues for subsequent models, making the models less robust.

However, to resolve the latter 2 issues, one can do dimensionality reduction. In [Assignment 1](https://github.com/huijunzhao-ds/cs224n/blob/master/a1/exploring_word_vectors.ipynb) we implement the co-occurrence matrix generation and applied SVD, one of the dimensionality reduction algorithms to obtain a low-dimensional representation. In a future post, we will elaborate more on dimensionality reduction.

### Optimization
Considering training efficiency, we apply techniques such as __Stochastic Gradient Descent (SGD)__ and __Negative Sampling__.

People working on deep learning should be pretty familiar with Gradient Descent. SGD is just an approximation of Gradient Descent by sampling batches from the training dataset and updating the trained parameters after each batch. It saves a lot of computational resources such as memory and CPU/GPU, and updates the model much faster. It is also very commonly used for almost all neural networks where the number of parameters or the size of training data is large.

Another computational consideration is that to compute $P(o\|c)$ (or $P(c\|o)$, where the computation expense is the same) we need to do a large number of $\exp()$ operations (depending on the size of the vocabulary), which is computational expensive. Instead, we train binary logistic regressions for a positive sample (the center word paired with an outside word) versus several negative samples (the center word paired with a random word), so that the softmax becomes sigmoid. Then the overall loss function becomes

$$ J=-\log(\sigma(u_o^Tv_c)) - \sum_{k=1}^K\mathbb{E}_{k\sim P(w)}\log(\sigma(-u_k^Tv_c)) $$

where $\sigma(x)=\frac{1}{1+e^{-x}}$ is the sigmoid function, $K$ is the number of negative samples, and $P(w)=U(w)^{3/4}$ denoting the unigram distribution $U(w)$ to $3/4$ power. The power here makes less frequent words sampled more often. In [Assignment 2](https://github.com/huijunzhao-ds/cs224n/tree/master/a2) we implement skip-gram with SGD and negative sampling. 

## NER
The task for NER is to find and classify names (into categories such as Person, Location, Organization, etc) in text based on a word2vec representation. The downstream purpose can be one of the following.
- Tracking mentions of particular entities in documents;
- Finding answers for Question-Answering (QA) tasks, since answers are usually named entities;
- Named entity linking/canonicalization into knowledge base;
- Similar techniques can be generalized to other slot-filling classifications.

There are several challenges for NER, including detecting entity boundaries, identifying entities, handling unknown or novel entities, and ambiguous or context-dependent entity classes. 

One way to deal with these challenges is the so-called __Word-Window Classification__, meaning that we classify a word in its context window. More explicitly, we take the concatenation of word vectors in a window surrounding the target word as the input to a neural network. A simple Multi-layer Perception (MLP) architecture with a softmax prediction function and cross-entropy loss is sufficient to do the job. 

As an alternative, we can also use the scores (outputs of MLP, without softmax) $s_t$ of the true window and $s_c$ of the corrupt window to compute the loss function as 

$$ J=\max(0,1-s_t+s_c) $$

Note that $J$ is not differentiable because of the $\max()$ operation. In fact, it is semi-differentiable, just as other commonly-used activation functions like ReLU. We can use SGD on semi-differentiable functions for optimization.